{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NewsWe advance RL. RL advances everything.","text":"[Research] ICML 2025 Oral <p>One member's paper is accepted as oral by ICML 2025!</p> [Research] ICCV 2025 <p>Three members' papers are accepted by ICCV 2025!</p> [New] Plasticine <p>Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning.</p> [TMLR] RLeXplore <p>Accelerating Research in Intrinsically-Motivated Reinforcement Learning.</p> [Research] ICLR 2025 Oral <p>LS-Imagine: Open-World Reinforcement Learning over Long Short-Term Imagination.</p> [Research] AAAI 2025 <p>One member's paper is accepted by AAAI 2025!</p>"},{"location":"about/","title":"About","text":"<p>The \u200b\u200bReinforcement Learning Evolution Foundation is a 501(c)(3) nonprofit organization dedicated to a bold vision: \u200b\u200bevolving reinforcement learning (RL) while using RL to drive progress across all fields of science, industry, and society.\u200b</p> <p>Unlike traditional foundations that focus on standardization and stability, \u200b\u200bwe embrace experimentation, radical innovation, and real-world impact.\u200b\u200b We believe RL's true potential lies not in rigid frameworks, but in \u200b\u200bbreaking boundaries\u200b\u200b\u2014whether in robotics, finance, or beyond.</p>"},{"location":"about/#annual-topic","title":"Annual Topic","text":"<p>We believe focused innovation drives meaningful progress. Each year, we select a \u200b\u200bgroundbreaking theme\u200b\u200b that pushes the boundaries of what RL can achieve. This annual focus allows us to \u200bcConcentrate resources\u200b\u200b on solving high-impact challenges through targeted research initiatives, and \u200b\u200bfoster interdisciplinary collaboration\u200b\u200b by bringing together domain experts and RL researchers.</p> <p>Annual Topic of 2025: Embodied Reinforcement Intelligence.</p>"},{"location":"about/#project-incubator","title":"Project Incubator","text":"<p>We actively seek out \u200b\u200bhigh-potential RL projects\u200b\u200b\u2014whether from independent researchers, startups, or academic labs\u2014and provide them with \u200b\u200bcomputing resources, mentorship, and funding\u200b\u200b to accelerate their growth. If your idea could redefine RL's future, \u200b\u200bwe want to hear about it.\u200b</p>"},{"location":"about/#contact-us","title":"Contact Us","text":"<p>To learn more about our vision and progress, we invite you to read our Events page. For partnership inquiries, funding opportunities, or project proposals, our team welcomes your emails at contact@rlevolution.org. We're particularly interested in hearing from researchers and organizations working on cutting-edge RL applications that align with our annual themes. </p>"},{"location":"donations/","title":"Donations","text":"<p>We welcome both financial contributions and compute power donations\u200b\u200b, which will be used to maintain our libraries and develop new ones.</p> <p>We are a 501c3 nonprofit, meaning that donations are deductible on your taxes!</p> <p>Please reach out via contact@rlevolution.org. We're looking forward to your emails!</p>"},{"location":"donations/#major-donors","title":"Major Donors","text":""},{"location":"events/","title":"Events","text":""},{"location":"events/#2025","title":"2025","text":"<ul> <li>One member's paper is accepted as oral by ICML 2025! See the post for details.</li> <li>Three members' papers are accepted by ICCV 2025! See the post for details.</li> <li>The annual topic of 2025 is announced: Embodied Reinforcement Intelligence.</li> <li>One member's paper is accepted as oral by ICLR 2025! See the post for details.</li> <li>We are announcing a project, Plasticine, which aims to accelerate research in plasticity-motivated deep RL.</li> <li>RLeXplore is accepted by the Transactions on Machine Learning Research, which provides high-quality implementations of intrinsic rewards in RL.</li> <li>One member's paper is accepted by AAAI 2025! See the post for details.</li> </ul>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#annual-topic-of-2025-embodied-reinforcement-intelligence","title":"Annual Topic of 2025: Embodied Reinforcement Intelligence","text":"[Undergoing] ERIC <p>Embodied Reinforcement IntelligenCe Framework.</p>"},{"location":"projects/#rl-research","title":"RL Research","text":"[Undergoing] Plasticine <p>Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning.</p> [ICLR2025 Oral] LS-Imagine <p>Open-World Reinforcement Learning over Long Short-Term Imagination.</p> [TMLR] RLeXplore <p>Accelerating Research in Intrinsically-Motivated Reinforcement Learning.</p> [TMLR] RLeXplore <p>Long-Term Evolution Project of Reinforcement Learning.</p>"},{"location":"projects/#rl-applications","title":"RL Applications","text":"FinMaster <p>Mastering Full-Pipeline Financial Workflows with LLMs.</p>"},{"location":"team/","title":"Steering Committee (in alphabetical order)","text":"Bo Li <p>Assistant Professor</p> <p>The Hong Kong Polytechnic University</p> Jiayu Chen <p>Assistant Professor</p> <p>The University of Hong Kong</p> Xin Jin <p>Assistant Professor</p> <p>Eastern Institute of Technology</p> Yunbo Wang <p>Associate Professor</p> <p>Shanghai Jiao Tong University</p> <p></p>"},{"location":"team/#core-team","title":"Core Team","text":"Mingqi Yuan <p>Founder</p> <p>PhD Student @ HKPU</p> Roger Creus Castanyer <p>Co-founder</p> <p>PhD student @ Mila / UdeM</p> Qi Wang <p>Co-founder</p> <p>PhD student @ SJTU</p> Guozheng Ma <p>Co-founder</p> <p>PhD student @ NTU</p> Junzhe Jiang <p>Chief Development Officer</p> <p>PhD student @ HKPU</p> Cheng Chen <p>Chief Media Officer</p> <p>PhD student @ HKPU</p>"},{"location":"posts/20250711_aaai/","title":"AAAI 2025 Paper","text":"<p>Congratulations to our CDO, Junzhe Jiang, for having his article accepted as an oral article by 2025 Annual AAAI Conference on Artificial Intelligence (AAAI)!</p> <p>The AAAI is a leading international academic conference in artificial intelligence held annually. It ranks 4th in terms of H5 Index in Google Scholar's list of top AI publications, after ICLR, NeurIPS, and ICML. It is supported by the Association for the Advancement of Artificial Intelligence. </p> <ul> <li>Title: </li> </ul> <p>Logic-Q: Improving Deep Reinforcement Learning-based Quantitative Trading via Program Sketch-based Tuning</p> <ul> <li>Abstract: </li> </ul> <p>Deep reinforcement learning (DRL) has revolutionized quantitative trading (Q-trading) by achieving decent performance without significant human expert knowledge. Despite its achievements, we observe that the current state-of-the-art DRL models are still ineffective in identifying the market trends, causing them to miss good trading opportunity or suffer from large drawdowns when encountering market crashes. To address this limitation, a natural approach is to incorporate human expert knowledge in identifying market trends. Whereas, such knowledge is abstract and hard to be quantified. In order to effectively leverage abstract human expert knowledge, in this paper, we propose a universal logic-guided deep reinforcement learning framework for Q-trading, called Logic-Q. In particular, Logic-Q adopts the program synthesis by sketching paradigm and introduces a logic-guided model design that leverages a lightweight, plug-and-play market trend-aware program sketch to determine the market trend and correspondingly adjusts the DRL policy in a post-hoc manner. Extensive evaluations of two popular quantitative trading tasks demonstrate that Logic-Q can significantly improve the performance of previous state-of-the-art DRL trading strategies.</p>"},{"location":"posts/20250811_iccv/","title":"ICCV 2025 Paper","text":"<p>Congratulations to our founder, Mingqi Yuan, and co-founders, Qi Wang and Guozheng Ma, for having their articles accepted by 2025 International Conference on Computer Vision (ICCV)!</p> <p>The ICCV is a research conference sponsored by the Institute of Electrical and Electronics Engineers (IEEE) held every other year. It is considered to be one of the top conferences in computer vision, alongside CVPR and ECCV, and it is held on years in which ECCV is not.</p>"},{"location":"posts/20250811_iccv/#paper-1-mingqi-yuan","title":"Paper 1 - Mingqi Yuan","text":"<ul> <li>Title: </li> </ul> <p>ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning</p> <ul> <li>Abstract: </li> </ul> <p>Hyperparameter optimization (HPO) is a billion-dollar problem in machine learning, which significantly impacts the training efficiency and model performance. However, achieving efficient and robust HPO in deep reinforcement learning (RL) is consistently challenging due to its high non-stationarity and computational cost. To tackle this problem, existing approaches attempt to adapt common HPO techniques (e.g., population-based training or Bayesian optimization) to the RL scenario. However, they remain sample-inefficient and computationally expensive, which cannot facilitate a wide range of applications. In this paper, we propose ULTHO, an ultra-lightweight yet powerful framework for fast HPO in deep RL within single runs. Specifically, we formulate the HPO process as a multi-armed bandit with clustered arms (MABC) and link it directly to long-term return optimization. ULTHO also provides a quantified and statistical perspective to filter the HPs efficiently. We test ULTHO on benchmarks including ALE, Procgen, MiniGrid, and PyBullet. Extensive experiments demonstrate that the ULTHO can achieve superior performance with a simple architecture, contributing to the development of advanced and automated RL systems.</p> <ul> <li>Code: </li> </ul> <p>Coming soon...</p>"},{"location":"posts/20250811_iccv/#paper-2-qi-wang","title":"Paper 2 - Qi Wang","text":"<ul> <li>Title: </li> </ul> <p>Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning</p> <ul> <li>Abstract: </li> </ul> <p>Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, \\textit{i.e.,} RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentanglement representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks.</p> <ul> <li>Code: </li> </ul> <p>Coming soon...</p>"},{"location":"posts/20250811_iccv/#paper-3-guozheng-ma","title":"Paper 3 - Guozheng Ma","text":"<ul> <li>Title: </li> </ul> <p>Faster and Better 3D Splatting via Group Training</p> <ul> <li>Abstract: </li> </ul> <p>3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis, demonstrating remarkable capability in high-fidelity scene reconstruction through its Gaussian primitive representations. However, the computational overhead induced by the massive number of primitives poses a significant bottleneck to training efficiency. To overcome this challenge, we propose Group Training, a simple yet effective strategy that organizes Gaussian primitives into manageable groups, optimizing training efficiency and improving rendering quality. This approach shows universal compatibility with existing 3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining superior synthesis quality. Extensive experiments reveal that our straightforward Group Training strategy achieves up to 30% faster convergence and improved rendering quality across diverse scenarios.</p> <ul> <li>Code: </li> </ul> <p>Coming soon...</p>"},{"location":"posts/20250812_icml/","title":"ICML 2025 Oral Paper","text":"<p>Congratulations to our co-founder, Guozheng Ma, for having his article accepted as an oral article by 2025 International Conference on Machine Learning (ICML)!</p> <p>The ICML is a leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three most respected conferences of high impact in machine learning and artificial intelligence research.</p> <ul> <li>Title: </li> </ul> <p>Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning</p> <ul> <li>Abstract: </li> </ul> <p>Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.</p> <ul> <li>Code: </li> </ul> <p>https://github.com/lilucse/SparseNetwork4DRL</p>"}]}